{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58159693-26b5-4083-a688-eaec9be32314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Retrieval-Augmented Generation on Scientific Publications\n",
    "\n",
    "### The goal of the project is to create a RAG model specifically for my own scientific publications and area of interest. Making sure that everything is in line with ownership rights I only use open access publications.\n",
    "\n",
    "The first task is to download the open access documents. For now the following papers are used:\n",
    "[1] Mihály Katona, Tamás Orosz, Robustness of a flux-intensifying permanent magnet-assisted synchronous reluctance machine focusing on shifted surface-inset ferrite magnets, Computers & Structures, Volume 316,2025,107845,ISSN 0045-7949, https://doi.org/10.1016/j.compstruc.2025.107845\n",
    "\n",
    "[2] Mihály Katona, Miklós Kuczmann, Tamás Orosz, Accuracy of the robust design analysis for the flux barrier modelling of an interior permanent magnet synchronous motor, Journal of Computational and Applied Mathematics, Volume 429, 2023, 115228, ISSN 0377-0427, https://doi.org/10.1016/j.cam.2023.115228\n",
    "\n",
    "[3] Mihály Katona, Tamás Orosz, Cogging Torque Reduction of a Flux-Intensifying Permanent Magnet-Assisted Synchronous Reluctance Machine with Surface-Inset Magnet Displacement, Energies 18, no. 20: 5492. https://doi.org/10.3390/en18205492\n",
    "\n",
    "### 1) The first step of data ingestion (bronze table) is to create a structured table from the papers called parsing. (NB_000_parsing)\n",
    "\n",
    "This table currently has one row per PDF. The most important column is parsed_output, which contains a complex \"Variant\" object (similar to a giant JSON tree) that holds the entire text and layout of your paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7da6e75f-19cc-4248-9d2d-acdd219ffa74",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "NB_000_parsing"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- This creates a 'Bronze' table containing the structured content of the papers using ai_parse_document function.\n",
    "CREATE OR REPLACE TABLE workspace.default.parsed_papers AS\n",
    "SELECT\n",
    "  path,\n",
    "  ai_parse_document(content, map('version', '2.0')) as parsed_output,\n",
    "  modificationTime\n",
    "FROM READ_FILES('/Volumes/workspace/default/publications', format => 'binaryFile');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93dc6191-2b8f-4d2e-834f-7a880de90dfd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "NB_001_parsing"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Return the parsed_papers table\n",
    "SELECT * FROM workspace.default.parsed_papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d7b76f7-2904-4dcc-9abd-44d406695836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2) The second step is to transform the data (silver table) with the goal to slice the data into smaller ones calld chunking.\n",
    "\n",
    "A single academic paper can have thousands of words. If it got fed into a chatbot, the bot will get lost in the data. It is needed to be broken into smaller pieces (chunks) while keeping the academic context (like which page or section the text came from).\n",
    "\n",
    "In PySpark, expr stands for Expression. Standard Python code doesn't understand the special Colon (:) syntax used to navigate Variant data. expr allows Spark to treat the code as SQL command.\n",
    "\n",
    "parsed_output:document.elements is the specific address of the research data inside the Variant.\n",
    "\n",
    "parsed_output is the main column where the parser stored everything. (:) is the key that opens the Variant data type. document.elements is the specific folder inside the paper that contains the list of paragraphs, tables, and headers (NB_010_chunking).\n",
    "\n",
    "The next to steps is exploding the parsed table to element level, to chunks (NB_011_chunking and NB_012_chunking). It is important to check how the parser sliced the documents. Whether those are sound contentwise. For example in this step the parses sliced the documents by paragraphs, which is a good start. Another important step in creating the silver table is deleting the NULL chunks and deleting the chunks which are less than X characters to reduce noise.\n",
    "\n",
    "### The minimal chunks size is one parameter to optimise the parsing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62ab6a4b-3567-4767-b79e-1249eb618974",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "NB_010_chunking"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, expr\n",
    "\n",
    "# Loading the bronze table with Spark.\n",
    "df_parsed = spark.table(\"workspace.default.parsed_papers\")\n",
    "\n",
    "# Adding the elements array column\n",
    "df_with_elements = df_parsed.select(\n",
    "    \"*\", \n",
    "    expr(\"parsed_output:document.elements\").alias(\"elements_array\")\n",
    ")\n",
    "\n",
    "display(df_with_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da4f453-afe4-4edb-882a-0fbd9a781a1b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "NB_011_chunking"
    }
   },
   "outputs": [],
   "source": [
    "# Exploding the elements array\n",
    "df_exploded = df_with_elements.lateralJoin(\n",
    "    spark.tvf.variant_explode(col(\"elements_array\"))\n",
    ")\n",
    "\n",
    "display(df_exploded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cc47122-f5e2-47bf-baa0-dc9cb515a7d5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "NB_012_chunking"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the chunks\n",
    "df_chunks = df_exploded.select(\n",
    "    \"path\",\n",
    "    expr(\"value:content\").alias(\"chunk_text\"),\n",
    "    expr(\"value:type\").alias(\"element_type\"),\n",
    ")\n",
    "\n",
    "display(df_chunks)\n",
    "\n",
    "print(f\"Total chunks created: {df_chunks.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd149e25-0c54-4d86-9e46-a249ddd1fb54",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770058342691}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "NB_013_chunking"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# The filtering step for NULL and chunk size.\n",
    "df_no_index = df_chunks.filter(\"chunk_text IS NOT NULL AND length(chunk_text) > 50\")\n",
    "\n",
    "# Adding a unique ID and casting chunk_text to string.\n",
    "df_silver = df_no_index.withColumn(\"chunk_id\", sf.uuid()).withColumn(\"chunk_text_string\", expr(\"chunk_text::string\")).withColumn(\"element_type_text\", expr(\"element_type::string\"))\n",
    "\n",
    "display(df_silver)\n",
    "\n",
    "print(f\"Total chunks after filtering: {df_silver.count()}\")\n",
    "\n",
    "df_silver.write.option('mergeSchema', 'true').mode(\"overwrite\").saveAsTable(\"workspace.default.chunked_papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df71dee8-2c35-4419-b7bc-1c4f4b347275",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The next step is to turn that static table into a Living Search Engine. We do this by creating a Mosaic AI Vector Search Index. This is the process where Databricks takes the table, converts it into high-dimensional math (vectors), and stores it so that the AI model can find the right needle in the academic haystack of papers in milliseconds.\n",
    "\n",
    "In the FREE version of Databricks follow these steps:\n",
    "\n",
    "1. Open Catalog Explorer: Navigate to chunked_papers table.\n",
    "2. In the top-right corner, click the Create button and select Vector Search Index.\n",
    "3. You should see the following UI.\n",
    "\n",
    "![image_1770057989192.png](./image_1770057989192.png \"image_1770057989192.png\")\n",
    "\n",
    "a) For the index name I chose puplication_index.\n",
    "\n",
    "b) Primary key: Select the column that uniquely identifies each chunk (ID).\n",
    "\n",
    "c) Columns to sync: Leave this blank to sync all columns. This ensures that it retrieves the original chunk_text, the path (for citations). It is important to cast VARIANT to STRING, because only STRING can be an input for embedding.\n",
    "\n",
    "d) Embedding source: Keep \"Compute embeddings\" selected. This tells Databricks to automatically turn your text into numbers using its built-in models.\n",
    "\n",
    "e) Embedding source column: Select chunk_text. This is the actual content the AI will \"read\" and vectorize for semantic searching. It is important to cast VARIANT to STRING, because only STRING can be an input for embedding.\n",
    "\n",
    "f) Embedding model: Choose databricks-gte-large-en. The databricks-gte-large-en is a text embedding model designed to map text into a 1024-dimensional vector representation. It supports an embedding window of up to 8192 tokens, making it suitable for tasks like retrieval, classification, clustering, semantic search, and question-answering. This model is particularly effective when paired with large language models (LLMs) for retrieval-augmented generation (RAG) use cases.\n",
    "\n",
    "g) Sync computed embeddings: Toggle this to ON if you want to save the generated numerical vectors into a separate table in Unity Catalog. This is very useful for advanced operations to perform similarity analysis or clustering later without re-computing the vectors.\n",
    "\n",
    "h) Vector search endpoint: Create one if you don't have it already. You should see the following:\n",
    "\n",
    "![image_1770059266585.png](./image_1770059266585.png \"image_1770059266585.png\")\n",
    "\n",
    "i) Sync mode: I recommend choosing Triggered sync mode. Unless you are uploading new research papers every few minutes and need the chatbot to know about them within seconds.\n",
    "\n",
    "You might encounter the following:\n",
    "\n",
    "Change Data Feed must be enabled to create an index from this table. See more details on Change Data Feed in the delta docs .\n",
    "Enabling Change Data Feed requires active compute. Please start a cluster with DBR version 11.3 and above or Pro/Serverless warehouse.\n",
    "\n",
    "Because Vector Search is a live system, it needs a way to watch your Silver Table for any new papers or updated paragraphs. Change Data Feed (CDF) is the security camera that allows Databricks to track every single row-level change without re-reading the entire table. Enable it with NB_014_chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a55b004-ac69-492e-95ae-e79fd235a23a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "NB_014_chunking"
    }
   },
   "outputs": [],
   "source": [
    "# Enable Change Data Feed\n",
    "spark.sql(\"ALTER TABLE workspace.default.chunked_papers SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
    "\n",
    "# Check if 'delta.enableChangeDataFeed' is now 'true'\n",
    "display(spark.sql(\"SHOW TBLPROPERTIES workspace.default.chunked_papers\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a77c90d-a456-4ae3-a56a-6d7c16df4794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### While the endpoint is under PROVISION (getting ready) it is recommended to create a TRUTH TABLE.\n",
    "The Truth Table is the source of truth for your AI. In the world of RAG (Retrieval-Augmented Generation), it is a carefully curated collection of questions and their perfect answers, validated by a human expert. With it, it is possible to mathematically approximate the models accuracy later.\n",
    "\n",
    "In MLFlow, a Truth Table isn't just a list of questions. Each row in your table should contain these specific fields:\n",
    "- inputs: The exact question a user might ask.\n",
    "- expected_retrieved_context: The specific chunk or page number where the answer is found.\n",
    "- expected_facts: A list of key facts that must appear in the answer for it to be considered correct.\n",
    "\n",
    "#### How to Build Your Truth Table Manually\n",
    "Open your research papers, pick a paragraph, and write a question that only that paragraph can answer. Include an unnanswerable questions. Ask something that is not in your papers. The AI should know how to say, \"I'm sorry, my research papers do not contain information on that topic,\" rather than making it up.\n",
    "\n",
    "#### How to Build Your Truth Table Syntheticly\n",
    "It uses the Mosaic AI Agent Evaluation library to automatically generate a Truth Table. Instead of you manually writing 20 questions and answers, this code uses an LLM to read the documents and invent the testcases. Of course, one should review these synthetic testcases manually and correct or delete if necessary before evaluating correctness or relevance.\n",
    "\n",
    "WARNING (The synthetic generation runs either way.): Failed to count tokens for text: Robustness of a flux-intensifying permanent magnet-assisted synchronous reluctance machine focusing on shifted surface-inset ferrite magnets. Error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')) --> The cluster is trying to reach an external URL to download the Tiktoken vocabulary file and is being cut off by a Databricks network security policy. Even though token counting seems like a local task, the library tiktoken (used by databricks-agents) does not ship with the vocabulary included. It tries to fetch it from openaipublic.blob.core.windows.net on its first run and fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64432331-edb7-4fb4-886a-85adb8320c9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# When building complex AI agents several cutting-edge libraries are being updated almost weekly. The -U (or --upgrade) flag is critical because the Mosaic AI Agent Framework is evolving rapidly. The agent is used for creating synthetic truth table.\n",
    "%pip install -U databricks-agents\n",
    "\n",
    "# The %pip commands do not automatically restart the Python interpreter. When a library gets installed, the files are downloaded to the cluster, but the Python kernel in memory is still holding onto the old versions. Restarting forces the interpreter to re-scan the packages and pick up the new versions.\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eb44237-935a-4a94-a45c-eedfe74993ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.agents.evals import generate_evals_df\n",
    "from pyspark.sql.functions import col, regexp_replace\n",
    "import warnings\n",
    "\n",
    "# Prepare the table for the generator\n",
    "df_for_eval = spark.table(\"workspace.default.chunked_papers\") \\\n",
    "    .filter(col(\"chunk_text_string\").isNotNull()) \\\n",
    "    .withColumn(\"content_clean\", regexp_replace(col(\"chunk_text_string\"), r'[^\\x00-\\x7F]+', ' ')) \\\n",
    "    .select(\n",
    "        col(\"content_clean\").alias(\"content\"),\n",
    "        col(\"chunk_id\").alias(\"doc_uri\")\n",
    "    ).toPandas()\n",
    "\n",
    "# Run the synthetic generator\n",
    "synthetic_evals = generate_evals_df(\n",
    "    df_for_eval,\n",
    "    # The number of evals to generate\n",
    "    num_evals=20,\n",
    "    # The agent description is used to specify the behaviour of the agent\n",
    "    agent_description=\"You are an expert robust design analysis of electric machines and circular economy principles. Also a an expert in retrieval augmented generation, so you are able to generate synthetic truth tables for a given prompt.\",\n",
    "    # The question guidelines are used to specify the type of questions to ask\n",
    "    question_guidelines=\"Ask technical questions that require specific values. Ask questions about the aim of the research and the conclusions too. Do not ask questions that are too broad or subjective.\"\n",
    ")\n",
    "\n",
    "# Convert to spark dataframe to access .write method\n",
    "df_truth = spark.createDataFrame(synthetic_evals)\n",
    "\n",
    "# Save to Unity Catalog\n",
    "df_truth.write.option(\"overwriteSchema\", \"true\").mode(\"overwrite\").saveAsTable(\"workspace.default.truth_table\")\n",
    "\n",
    "display(df_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7f08996-6259-42c4-9571-5a6abff70567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define the list of IDs to remove\n",
    "rows_to_remove = [\n",
    "    '717e211b5073788ee11854d1d10e79b9eb805275ccf8718f710da1c79746e455',\n",
    "    '736adc2c7cc82164bc51f276e95af55ebaec8bab994b891fe7642f470ab26a8e',\n",
    "    '311d9ec365f1371236c949bd990c5b60b58182fefd6d0e1da9ba8434dd24c045',\n",
    "    'f934e33ca2b303b39edbbd53e8cf7f0cf13caa1220c3ea566850b8a3cb38e270'\n",
    "]\n",
    "\n",
    "# Load the full table\n",
    "df_truth = spark.table(\"workspace.default.truth_table\")\n",
    "\n",
    "# Apply the filter (WHERE request_id NOT IN ...)\n",
    "df_truth_filtered = df_truth.filter(~col(\"request_id\").isin(rows_to_remove))\n",
    "\n",
    "# Save to Unity Catalog\n",
    "df_truth_filtered.write.option(\"overwriteSchema\", \"true\").mode(\"overwrite\").saveAsTable(\"workspace.default.truth_table_filtered\")\n",
    "\n",
    "display(df_truth_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1de0743-66bb-4cbb-bca8-e213fd5cf522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Here the synthetic truth table is converted into a digestable format for MLflow to evaluate.\n",
    "# The generate_evals_df function creates data in a complex chat protocol format (simulating a full conversation history), but MLflow evaluation setup requires a simple input map (a dictionary with a single \"query\" key).\n",
    "from pyspark.sql.functions import col, create_map, lit\n",
    "\n",
    "df_truth_filtered = spark.table(\"workspace.default.truth_table_filtered\")\n",
    "\n",
    "# Extracting the clean question and expectations\n",
    "df_flat = df_truth_filtered.select(\n",
    "    create_map(\n",
    "        lit(\"query\"), col(\"inputs\").messages.content[0]\n",
    "    ).alias(\"inputs\"),\n",
    "    col(\"expectations\")\n",
    ")\n",
    "\n",
    "df_flat.write.option(\"overwriteSchema\", \"true\").mode(\"overwrite\").saveAsTable(\"workspace.default.truth_table_converted\")\n",
    "\n",
    "display(df_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33d2eef5-8765-4f28-b71b-dd50301bc15c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### The following snippet is the evaluation for correctness and relevance.\n",
    "\n",
    "Correctness: \"Is the answer factually accurate?\" This metric measures accuracy by comparing your bot's answer against the Ground Truth (the expected_facts you defined in your truth table). Input: The bot's response + The expected_facts. The Judge asks: \"Does the response contain all the critical facts listed in the expectation? Is it factually true?\" Fails if: The bot says \"300 Nm torque\" when the truth table says \"350 Nm torque\". Passes if: The bot says \"The torque is 350 Nm,\" even if the wording is different from your expectation.\n",
    "\n",
    "Relevance (relevance_to_query)\n",
    "\"Did the bot actually answer the question?\" This metric measures the quality of the response in relation to the User's Query, ignoring the ground truth. It checks if the bot stayed on topic or hallucinated a refusal. Input: The bot's response + The original query. The Judge asks: \"Is this response helpful? Does it directly address the user's intent? Does it avoid rambling about unrelated topics?\" Fails if: The user asks \"What is the flux density?\" and the bot replies, \"Here is a summary of the circular economy.\" (This is irrelevant, even if the summary is factually true). Passes if: The bot provides a direct, concise answer to the specific question asked.\n",
    "\n",
    "| Scenario | Correctness | Relevance | Diagnosis |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **The Gold Standard** | ✅ High | ✅ High | The bot gave the right answer to the right question. |\n",
    "| **The \"Hallucination\"** | ❌ Low | ✅ High | The bot gave a direct answer (\"The value is 5\"), but it was **wrong** (Ground truth was 10). |\n",
    "| **The \"Politician\"** | ✅ High | ❌ Low | The bot said something true (\"The sky is blue\"), but it **didn't answer the question** about electric motors. |\n",
    "| **The \"Refusal\"** | ❌ Low | ❌ Low | The bot said \"I don't know\" or gave a completely broken response. |\n",
    "\n",
    "### --- PART 1: The Retrieval & Generation Logic ---\n",
    "#### Initialize the Vector Search Client\n",
    "This block establishes the **\"Handshake\"** with the Vector Database. Before we can search for answers, we must authenticate with the `VectorSearchClient` and point it to the exact location of our embedded knowledge (`publication_index`). This creates an `index` object that acts as the **Retriever**, allowing the bot to access the stored PDF chunks during the query phase.\n",
    "\n",
    "#### A. RETRIEVAL: Find the top 5 most similar chunks\n",
    "This is the core \"Search\" mechanism of the RAG pipeline. Instead of matching exact keywords (like Ctrl+F), we perform a **Similarity Search**. The `query_text` (user's question) is converted into a vector, and the database finds the top 5 chunks of text that are mathematically \"closest\" in meaning. We request both the content (`chunk_text_string`) and the source filename (`path`) so the LLM knows *what* to say and *where* it came from. The raw search results come back as a list of separate text chunks. To feed this information into the LLM, we must **flatten** them into a single string. This one-liner extracts just the text content (ignoring metadata for now) from the top 5 results and joins them together with newlines. This creates the `context` block, a unified paragraph of background knowledge, that we will paste into the prompt so the LLM has the \"facts\" it needs to answer the question.\n",
    "\n",
    "#### B. GENERATION: Send context + question to the LLM\n",
    "This is where the retrieved information is synthesized into a human-readable answer. We construct a prompt that strictly combines the **Context** (our PDF data) and the **Question** (the user's query). By sending this to the **Llama 3** model via the Databricks Serving Endpoint, we force the AI to \"ground\" its answer in our specific documents rather than its general training data. Finally, we return **both** the generated answer and the original context so the automated Judge can later verify if the answer was faithful to the source material.\n",
    "\n",
    "#### WRAPPER: This is the function MLflow will call to evaluate\n",
    "This function acts as the **\"Bridge\"** between our custom logic and the MLflow evaluation harness. MLflow expects a specific input/output format to run its automated tests. We wrap our `research_assistant` function here to:\n",
    "1.  **Standardize Inputs:** Accept a single `query` argument (which MLflow unpacks automatically).\n",
    "2.  **Format Outputs:** Return a dictionary with clearly labeled keys (`response`, `retrieved_context`) so the Judge knows exactly which text to grade.\n",
    "3.  **Enable Tracing:** The `@mlflow.trace` decorator turns on detailed logging, allowing us to see every step of the retrieval and generation process in the MLflow UI for debugging.\n",
    "\n",
    "### --- PART 2: The Data Sanitizer ---\n",
    "This block is a critical safety mechanism against serialization errors. When we convert Spark tables to Pandas, data is often stored as NumPy arrays (e.g., `np.ndarray`). However, MLflow's evaluation tools rely on standard JSON for logging, which **crashes** if it encounters NumPy types. This recursive function walks through every item in our dataset and converts any arrays into standard Python lists, ensuring the evaluation run completes without a serialization error.\n",
    "\n",
    "### --- PART 3: The Evaluation Execution ---\n",
    "This is the final step where we actually run the test. \n",
    "\n",
    "By calling `mlflow.genai.evaluate`, we orchestrate the entire test:\n",
    "1.  **Feed the Data:** It takes every question from our sanitized truth table (`eval_data`).\n",
    "2.  **Run the Agent:** It passes each question to our `eval_predict_fn` to get the real-time answer and context.\n",
    "3.  **Grade the Results:** It employs an \"LLM-as-a-Judge\" (a separate AI model) to score the answers based on **Correctness** (accuracy against ground truth) and **Relevance** (adherence to the query).\n",
    "4.  **Log Everything:** The `start_run()` context ensures every input, output, and score is recorded in the MLflow Experiment for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d37ab57d-508d-40e4-96d8-3af93f4d55aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-vectorsearch\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39b81aa1-1b26-48c8-9269-532305e003c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.deployments import get_deploy_client\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "import numpy as np\n",
    "\n",
    "# --- PART 1: The Retrieval & Generation Logic ---\n",
    "def research_assistant(question):\n",
    "    \"\"\"\n",
    "    This is the core logic of your RAG bot.\n",
    "    1. It searches your Vector Index for relevant PDF chunks.\n",
    "    2. It sends those chunks + the question to Llama 3.\n",
    "    \"\"\"\n",
    "    # Initialize the Vector Search Client\n",
    "    vsc = VectorSearchClient(disable_notice=True)\n",
    "    index = vsc.get_index(\n",
    "        endpoint_name=\"academic_search_endpoint\", \n",
    "        index_name=\"workspace.default.publication_index\"\n",
    "    )\n",
    "    \n",
    "# A. RETRIEVAL: Find the top 10 most similar chunks\n",
    "    results = index.similarity_search(\n",
    "        query_text=question,\n",
    "        columns=[\"chunk_text_string\", \"path\"],\n",
    "        num_results=10\n",
    "    )\n",
    "\n",
    "    # Combine the retrieved text into a single string for the prompt\n",
    "    context = \"\\n\".join([row[0] for row in results['result']['data_array']])\n",
    "    \n",
    "# B. GENERATION: Send context + question to the LLM\n",
    "    client = get_deploy_client(\"databricks\")\n",
    "\n",
    "    # 1. Define the System Instructions (Guardrails + CoT)\n",
    "    system_prompt = \"\"\"You are a technical research assistant in robust design optimisation, tolerance analysis and circular economy related topics.  Answer the question ONLY using the provided Context. If the answer is not in the context, say 'I do not have enough information in the provided documents to answer this.'\n",
    "\n",
    "    Follow this thinking process:\n",
    "    1. Extract relevant technical quotes from the context.\n",
    "    2. Reason step-by-step how these facts answer the user's question.\n",
    "    3. Provide a final concise answer.\"\"\"\n",
    "\n",
    "    # 2. Define Few-Shot Examples (Teaches the model the 'style' of answer)\n",
    "    few_shot_examples = [\n",
    "        {\"role\": \"user\", \"content\": \"Context: [Chunk 1: Skewing reduces Cogging Torque by 15%] Question: How to fix cogging?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Thinking: Context mentions skewing as a solution. \\nAnswer: According to the documents, skewing can reduce cogging torque by 15%.\"}\n",
    "    ]\n",
    "\n",
    "    # 3. Build the final message list\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        *few_shot_examples,\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {question}\"}\n",
    "    ]\n",
    "\n",
    "    response = client.predict(\n",
    "        endpoint=\"databricks-meta-llama-3-3-70b-instruct\",\n",
    "        inputs={\"messages\": messages}\n",
    "    )\n",
    "\n",
    "    answer = response['choices'][0]['message']['content']\n",
    "    \n",
    "    # Return both so the Judge can check if the answer matches the context\n",
    "    return answer, context\n",
    "\n",
    "# C. WRAPPER: This is the function MLflow will call to evaluate\n",
    "@mlflow.trace # Enables detailed tracing in the MLflow UI\n",
    "def eval_predict_fn(query):\n",
    "    \"\"\"\n",
    "    The 'predict_fn' for mlflow.evaluate must accept a single input row.\n",
    "    MLflow automatically unpacks the 'query' key from your table \n",
    "    and passes it here.\n",
    "    \"\"\"\n",
    "\n",
    "    answer, context = research_assistant(query)\n",
    "    \n",
    "    # We return a dictionary so MLflow can map these to metrics\n",
    "    return {\n",
    "        \"response\": answer,\n",
    "        \"retrieved_context\": context\n",
    "    }\n",
    "\n",
    "# --- PART 2: The Data Sanitizer ---\n",
    "def sanitize_for_json(obj):\n",
    "    \"\"\"\n",
    "    Recursively converts Numpy arrays and other non-JSON types \n",
    "    into standard Python lists and dictionaries.\n",
    "    Essential for preventing 'Not JSON Serializable' errors in MLflow.\n",
    "    \"\"\"\n",
    "    # If it's a Numpy array (common in Pandas), turn it into a list\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return [sanitize_for_json(i) for i in obj.tolist()]\n",
    "    \n",
    "    # If it's a list, check every item inside it\n",
    "    if isinstance(obj, list):\n",
    "        return [sanitize_for_json(i) for i in obj]\n",
    "    \n",
    "    # If it's a dictionary, check every value inside it\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: sanitize_for_json(v) for k, v in obj.items()}\n",
    "    \n",
    "    # Otherwise, it's safe (string, int, float), just return it\n",
    "    return obj\n",
    "\n",
    "# Load the flattened table we created earlier\n",
    "eval_data = spark.table(\"workspace.default.truth_table_converted\").toPandas()\n",
    "\n",
    "# Apply the sanitizer to the entire Pandas DataFrame\n",
    "eval_data[\"expectations\"] = eval_data[\"expectations\"].apply(sanitize_for_json)\n",
    "\n",
    "# --- PART 3: The Evaluation Execution ---\n",
    "with mlflow.start_run():\n",
    "    \n",
    "    # Run the evaluation harness\n",
    "    eval_results = mlflow.genai.evaluate(\n",
    "        data=eval_data,              # The sanitized Truth Table\n",
    "        predict_fn=eval_predict_fn,  # Your wrapped agent function\n",
    "        scorers=[\n",
    "            # 1. Correctness: Does the answer match the expected_facts?\n",
    "            mlflow.genai.scorers.Correctness(metric_name=\"answer_correctness\"),\n",
    "            \n",
    "            # 2. Relevance: Is the answer actually supported by the retrieved context?\n",
    "            mlflow.genai.scorers.RelevanceToQuery(metric_name=\"context_relevance\")\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f954776e-6450-4fc8-9ecc-1454a3026044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Analysis of Evaluation Traces\n",
    "The trace table provides a row-by-row breakdown of how the agent handled specific queries:\n",
    "\n",
    "Consistency in Relevance: The system has a 100% pass rate for Relevance across all visible rows (e.g., tr-ff14bae..., tr-9bb01b...). This indicates that your prompt engineering is successful at keeping the LLM focused on the questions asked.\n",
    "\n",
    "Correctness Failures: Several traces are marked as Fail for Correctness (shown in red), such as:\n",
    "\n",
    "tr-9bb01b... (\"What are the effects of combining...\")\n",
    "\n",
    "tr-d947ef2... (\"What specific factors are isolated...\")\n",
    "\n",
    "tr-7f37a08... (\"What is the range of torque-ripple...\")\n",
    "\n",
    "Performance Stability: Execution times are mostly stable, ranging from 1.1s to 5.8s. The longest execution (tr-76a47ab... at 5.8s) passed both metrics, suggesting that complexity doesn't necessarily correlate with failure.\n",
    "\n",
    "![image_1771269233157.png](./image_1771269233157.png \"image_1771269233157.png\")\n",
    "\n",
    "#### To improve Correctness in a RAG system, you must systematically address two areas: Retrieval (finding the right facts) and Generation (using those facts correctly). A failure in correctness often means the model is either missing the necessary information or ignoring the context to rely on its own training data.\n",
    "\n",
    "1. Optimize Retrieval (Fixing the \"Facts\")\n",
    "If the correctness score is low, the first step is ensuring the LLM is actually receiving the correct information.\n",
    "\n",
    "Adjust Chunking Strategy: Increase the chunk size or add chunk overlap to ensure sentences aren't cut off mid-thought, which can lead to incomplete facts being fed to the model. (`--> The ai_parse_document function itself does not have native parameters for chunk size or overlap. This is because its primary job is structural parsing (extracting tables, text, and layout) rather than chunking. So a post processing step must be implemented using LangChain.`)\n",
    "\n",
    "Increase top-k: Increasing the number of retrieved chunks (e.g., from 5 to 10) can improve \"context sufficiency,\" giving the model a better chance of finding the specific detail needed for the answer. (`--> I implemented it first and it increased the correctness to 75%.`)\n",
    "\n",
    "Implement Re-ranking: Use a cross-encoder re-ranker after your initial vector search. This secondary step re-scores the retrieved chunks more precisely, ensuring the most relevant facts are at the top of the context. (`--> If you are using Mosaic AI Vector Search, you can implement reranking with a single line of code using the built-in DatabricksReranker. When you set this to hybrid, Databricks runs two searches in parallel and then fuses the results: Semantic Search (Dense Vector): The AI understands the meaning of your query. For \"How to reduce torque ripple,\" it might find documents about \"harmonics mitigation\" or \"skewing techniques,\" even if they don't use the exact word \"ripple\". Keyword Search (Lexical): It looks for exact word matches. This is critical for technical terms like \"PMSM,\" \"LMP-9R2,\" or specific error codes that a vector model might find \"similar\" to other unrelated terms. *Error: Reranking is not yet enabled for this workspace. Please contact support if you are interested in this feature.*`)\n",
    "\n",
    "2. Refine Generation (Fixing the \"Response\") (`-->I replaced the naive prompt with a structured system prompt that includes  instructions, few-shot examples, and Chain-of-Thought (CoT) triggers. It did not change correctness score, however, with the new system prompts it discovered a genuine misinterpretation in the paper.`)\n",
    "\n",
    "Even with perfect retrieval, the LLM may struggle to process the information accurately.\n",
    "\n",
    "Prompt Engineering: Use explicit instructions such as \"Answer only using the provided context\" or \"If the information is not present, say you do not know\" to reduce guessing and hallucinations.\n",
    "\n",
    "Few-Shot Prompting: Provide 2–3 examples of \"Question + Context + Correct Answer\" within your prompt to show the model exactly how you want it to extract and format technical data.\n",
    "\n",
    "Chain-of-Thought (CoT): Instruct the model to \"Think step-by-step\" or \"Extract relevant quotes first before answering.\" This forces the model to reason through the context before finalizing its response.\n",
    "\n",
    "3. Systematic Debugging with MLflow\n",
    "Since you are using MLflow, leverage its diagnostic tools to pinpoint the failure:\n",
    "\n",
    "Inspect Traces: Use the MLflow UI to look at the retrieved context for a failed row. If the answer isn't in those chunks, the problem is Retrieval; if the answer is there but the model missed it, the problem is Generation. (`--> With the new system prompts the AI discovered a genuine misinterpretation in the paper. Correctness should always be checked manually!`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f72718a2-25d4-4a2b-8800-4e351a0c9574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad8e21da-2503-46fe-977f-eb5c748f68ac",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771324053591}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from mlflow.deployments import get_deploy_client\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "def research_assistant(question):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame where each row is a retrieved chunk, \n",
    "    allowing you to see exactly which text block came from which file.\n",
    "    \"\"\"\n",
    "    # Initialize Clients\n",
    "    vsc = VectorSearchClient(disable_notice=True)\n",
    "    index = vsc.get_index(\n",
    "        endpoint_name=\"academic_search_endpoint\", \n",
    "        index_name=\"workspace.default.publication_index\"\n",
    "    )\n",
    "    \n",
    "    # A. RETRIEVAL: Find top 10 chunks\n",
    "    results = index.similarity_search(\n",
    "        query_text=question,\n",
    "        columns=[\"chunk_text_string\", \"path\"],\n",
    "        num_results=10,\n",
    "        query_type=\"hybrid\"\n",
    "    )\n",
    "    \n",
    "    # raw_data is a list of lists: [[text, path], [text, path], ...]\n",
    "    raw_data = results['result']['data_array']\n",
    "    \n",
    "    # Prepare single string context for the LLM\n",
    "    context_text = \"\\n\\n\".join([row[0] for row in raw_data])\n",
    "    \n",
    "    # B. GENERATION\n",
    "    client = get_deploy_client(\"databricks\")\n",
    "\n",
    "    system_prompt = \"\"\"You are a technical research assistant in robust design optimisation. \n",
    "    Be creative based on the given context.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {context_text}\\n\\nQuestion: {question}\"}\n",
    "    ]\n",
    "\n",
    "    response = client.predict(\n",
    "        endpoint=\"databricks-meta-llama-3-3-70b-instruct\",\n",
    "        inputs={\"messages\": messages}\n",
    "    )\n",
    "\n",
    "    answer = response['choices'][0]['message']['content']\n",
    "    \n",
    "    # C. FORMAT AS TABLE (One row per chunk)\n",
    "    # 1. Create DataFrame from the raw list of chunks and paths\n",
    "    # The Similarity Score is a numerical metric that tells you how closely a retrieved document matches your query. In Databricks Vector Search, this score represents the \"distance\" or \"relevance\" between your question's vector and the document's vector.\n",
    "    df = pd.DataFrame(raw_data, columns=[\"Chunks\", \"Path\", \"Similarity\"])\n",
    "    \n",
    "    # 2. Add the generated Answer as a new column (repeated for every row)\n",
    "    df.insert(0, \"Answer\", answer)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test it\n",
    "df_result = research_assistant(\"How Taguchi method is used for robust design?\")\n",
    "display(df_result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7181965510664607,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "research_assistant",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
